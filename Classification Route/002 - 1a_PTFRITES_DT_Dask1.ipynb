{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "940ca756-c585-45af-a796-e4766fe5fa75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dask-ml\n",
      "  Downloading dask_ml-2024.4.4-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting dask-glm>=0.2.0 (from dask-ml)\n",
      "  Downloading dask_glm-0.3.2-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: dask>=2.4.0 in /opt/anaconda3/lib/python3.11/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (2023.11.0)\n",
      "Requirement already satisfied: distributed>=2.4.0 in /opt/anaconda3/lib/python3.11/site-packages (from dask-ml) (2023.11.0)\n",
      "Requirement already satisfied: multipledispatch>=0.4.9 in /opt/anaconda3/lib/python3.11/site-packages (from dask-ml) (0.6.0)\n",
      "Requirement already satisfied: numba>=0.51.0 in /opt/anaconda3/lib/python3.11/site-packages (from dask-ml) (0.59.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /opt/anaconda3/lib/python3.11/site-packages (from dask-ml) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.11/site-packages (from dask-ml) (23.1)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /opt/anaconda3/lib/python3.11/site-packages (from dask-ml) (2.1.4)\n",
      "Requirement already satisfied: scikit-learn>=1.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from dask-ml) (1.2.2)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.11/site-packages (from dask-ml) (1.11.4)\n",
      "Requirement already satisfied: click>=8.1 in /opt/anaconda3/lib/python3.11/site-packages (from dask>=2.4.0->dask[array,dataframe]>=2.4.0->dask-ml) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle>=1.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from dask>=2.4.0->dask[array,dataframe]>=2.4.0->dask-ml) (2.2.1)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /opt/anaconda3/lib/python3.11/site-packages (from dask>=2.4.0->dask[array,dataframe]>=2.4.0->dask-ml) (2023.10.0)\n",
      "Requirement already satisfied: partd>=1.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from dask>=2.4.0->dask[array,dataframe]>=2.4.0->dask-ml) (1.4.1)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /opt/anaconda3/lib/python3.11/site-packages (from dask>=2.4.0->dask[array,dataframe]>=2.4.0->dask-ml) (6.0.1)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /opt/anaconda3/lib/python3.11/site-packages (from dask>=2.4.0->dask[array,dataframe]>=2.4.0->dask-ml) (0.12.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from dask>=2.4.0->dask[array,dataframe]>=2.4.0->dask-ml) (7.0.1)\n",
      "Collecting sparse>=0.7.0 (from dask-glm>=0.2.0->dask-ml)\n",
      "  Downloading sparse-0.15.1-py2.py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in /opt/anaconda3/lib/python3.11/site-packages (from distributed>=2.4.0->dask-ml) (3.1.3)\n",
      "Requirement already satisfied: locket>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from distributed>=2.4.0->dask-ml) (1.0.0)\n",
      "Requirement already satisfied: msgpack>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from distributed>=2.4.0->dask-ml) (1.0.3)\n",
      "Requirement already satisfied: psutil>=5.7.2 in /opt/anaconda3/lib/python3.11/site-packages (from distributed>=2.4.0->dask-ml) (5.9.0)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.5 in /opt/anaconda3/lib/python3.11/site-packages (from distributed>=2.4.0->dask-ml) (2.4.0)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from distributed>=2.4.0->dask-ml) (1.7.0)\n",
      "Requirement already satisfied: tornado>=6.0.4 in /opt/anaconda3/lib/python3.11/site-packages (from distributed>=2.4.0->dask-ml) (6.3.3)\n",
      "Requirement already satisfied: urllib3>=1.24.3 in /opt/anaconda3/lib/python3.11/site-packages (from distributed>=2.4.0->dask-ml) (2.0.7)\n",
      "Requirement already satisfied: zict>=3.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from distributed>=2.4.0->dask-ml) (3.0.0)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.11/site-packages (from multipledispatch>=0.4.9->dask-ml) (1.16.0)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /opt/anaconda3/lib/python3.11/site-packages (from numba>=0.51.0->dask-ml) (0.42.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas>=0.24.2->dask-ml) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas>=0.24.2->dask-ml) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas>=0.24.2->dask-ml) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn>=1.2.0->dask-ml) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn>=1.2.0->dask-ml) (2.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda3/lib/python3.11/site-packages (from importlib-metadata>=4.13.0->dask>=2.4.0->dask[array,dataframe]>=2.4.0->dask-ml) (3.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2>=2.10.3->distributed>=2.4.0->dask-ml) (2.1.3)\n",
      "Downloading dask_ml-2024.4.4-py3-none-any.whl (149 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.8/149.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dask_glm-0.3.2-py2.py3-none-any.whl (13 kB)\n",
      "Downloading sparse-0.15.1-py2.py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sparse, dask-glm, dask-ml\n",
      "Successfully installed dask-glm-0.3.2 dask-ml-2024.4.4 sparse-0.15.1\n"
     ]
    }
   ],
   "source": [
    "!pip install dask-ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27365357-399f-497f-ae72-52b02e818c33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "import dask.dataframe as dd\n",
    "from dask_ml.model_selection import train_test_split, GridSearchCV\n",
    "from dask_ml.wrappers import ParallelPostFit\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from dask_ml.preprocessing import OneHotEncoder\n",
    "from dask_ml.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2028a70-5755-4456-bef2-399236f3d5f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 54966 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Start a Dask client to manage the cluster\n",
    "client = Client()\n",
    "\n",
    "# Example data types and reading CSV with Dask\n",
    "# Define the data types for each column to optimize memory usage\n",
    "dtype_spec = {\n",
    "    'Em_on_target': 'category',\n",
    "    'Fuel consumption': 'float32',\n",
    "    'Engine_cm3': 'float32',\n",
    "    'Electric range (km)': 'float32',\n",
    "    'Kg_veh': 'float32',\n",
    "    'Test_mass': 'float32',\n",
    "    'Power_KW': 'float32',\n",
    "    'El_Consumpt_whkm': 'float32',\n",
    "    'Energy': 'category',\n",
    "    'Fuel_mode': 'category',\n",
    "    'Brand': 'category',\n",
    "    'Veh_type': 'category',\n",
    "    'Veh_Model': 'category'\n",
    "}\n",
    "columns = ['Em_on_target', 'Fuel consumption', 'Engine_cm3', 'Electric range (km)', 'Kg_veh', 'Test_mass', 'Power_KW', 'El_Consumpt_whkm', 'Energy', 'Fuel_mode', 'Brand', 'Veh_type', 'Veh_Model']\n",
    "dfdt = dd.read_csv('/Users/livalacaisse/Documents/DataScience/CO2/000-C02 First Delivery/Concatenate/PT_FR_ES_IT.csv', usecols=columns, dtype=dtype_spec, assume_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a817095-7386-4bdc-b0e6-82da03778a7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocessing steps using Dask-ML\n",
    "categorical_features = ['Energy', 'Fuel_mode', 'Brand', 'Veh_type', 'Veh_Model']\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=True)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', one_hot_encoder, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42faae5f-1d0f-4e1e-b423-fd5d7c013758",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a pipeline that works with Dask\n",
    "pipeline = make_pipeline(\n",
    "    preprocessor,\n",
    "    ParallelPostFit(DecisionTreeClassifier(random_state=42))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "936bca7c-5a4b-455c-8cb7-93197a14f2f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-01 23:07:27,317 - distributed.protocol.core - CRITICAL - Failed to deserialize\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/distributed/protocol/core.py\", line 160, in loads\n",
      "    return msgpack.loads(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/msgpack/fallback.py\", line 128, in unpackb\n",
      "    ret = unpacker._unpack()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/msgpack/fallback.py\", line 565, in _unpack\n",
      "    ret.append(self._unpack(EX_CONSTRUCT))\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/msgpack/fallback.py\", line 592, in _unpack\n",
      "    ret[key] = self._unpack(EX_CONSTRUCT)\n",
      "    ~~~^^^^^\n",
      "TypeError: unhashable type: 'dict'\n",
      "2024-05-01 23:07:27,319 - distributed.core - ERROR - Exception while handling op register-client\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/distributed/core.py\", line 968, in _handle_comm\n",
      "    result = await result\n",
      "             ^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/distributed/scheduler.py\", line 5532, in add_client\n",
      "    await self.handle_stream(comm=comm, extra={\"client\": client})\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/distributed/core.py\", line 1023, in handle_stream\n",
      "    msgs = await comm.read()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 248, in read\n",
      "    msg = await from_frames(\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/distributed/comm/utils.py\", line 78, in from_frames\n",
      "    res = _from_frames()\n",
      "          ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/distributed/comm/utils.py\", line 61, in _from_frames\n",
      "    return protocol.loads(\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/distributed/protocol/core.py\", line 160, in loads\n",
      "    return msgpack.loads(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/msgpack/fallback.py\", line 128, in unpackb\n",
      "    ret = unpacker._unpack()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/msgpack/fallback.py\", line 565, in _unpack\n",
      "    ret.append(self._unpack(EX_CONSTRUCT))\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/msgpack/fallback.py\", line 592, in _unpack\n",
      "    ret[key] = self._unpack(EX_CONSTRUCT)\n",
      "    ~~~^^^^^\n",
      "TypeError: unhashable type: 'dict'\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-187825' coro=<Server._handle_comm() done, defined at /opt/anaconda3/lib/python3.11/site-packages/distributed/core.py:874> exception=TypeError(\"unhashable type: 'dict'\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/distributed/core.py\", line 968, in _handle_comm\n",
      "    result = await result\n",
      "             ^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/distributed/scheduler.py\", line 5532, in add_client\n",
      "    await self.handle_stream(comm=comm, extra={\"client\": client})\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/distributed/core.py\", line 1023, in handle_stream\n",
      "    msgs = await comm.read()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 248, in read\n",
      "    msg = await from_frames(\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/distributed/comm/utils.py\", line 78, in from_frames\n",
      "    res = _from_frames()\n",
      "          ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/distributed/comm/utils.py\", line 61, in _from_frames\n",
      "    return protocol.loads(\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/distributed/protocol/core.py\", line 160, in loads\n",
      "    return msgpack.loads(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/msgpack/fallback.py\", line 128, in unpackb\n",
      "    ret = unpacker._unpack()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/msgpack/fallback.py\", line 565, in _unpack\n",
      "    ret.append(self._unpack(EX_CONSTRUCT))\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/msgpack/fallback.py\", line 592, in _unpack\n",
      "    ret[key] = self._unpack(EX_CONSTRUCT)\n",
      "    ~~~^^^^^\n",
      "TypeError: unhashable type: 'dict'\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "('split-0-e4130aa2d0d7089869f5d3cae5a12f73', 37)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mfit(X_train\u001b[38;5;241m.\u001b[39mcompute(), y_train\u001b[38;5;241m.\u001b[39mcompute())  \u001b[38;5;66;03m# Compute turns Dask DataFrame into a pandas DataFrame if necessary\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Predict and evaluate using Dask\u001b[39;00m\n\u001b[1;32m     10\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/dask/base.py:342\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    319\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \n\u001b[1;32m    321\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 342\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m compute(\u001b[38;5;28mself\u001b[39m, traverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/dask/base.py:628\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    625\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[0;32m--> 628\u001b[0m     results \u001b[38;5;241m=\u001b[39m schedule(dsk, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/distributed/client.py:2245\u001b[0m, in \u001b[0;36mClient._gather\u001b[0;34m(self, futures, errors, direct, local_worker)\u001b[0m\n\u001b[1;32m   2243\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2244\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exception\u001b[38;5;241m.\u001b[39mwith_traceback(traceback)\n\u001b[0;32m-> 2245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m   2246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2247\u001b[0m     bad_keys\u001b[38;5;241m.\u001b[39madd(key)\n",
      "\u001b[0;31mCancelledError\u001b[0m: ('split-0-e4130aa2d0d7089869f5d3cae5a12f73', 37)"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "X = dfdt.drop('Em_on_target', axis=1)\n",
    "y = dfdt['Em_on_target'].astype('int')  # Ensure correct type for the target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,shuffle=True)\n",
    "\n",
    "# Fit the model\n",
    "pipeline.fit(X_train.compute(), y_train.compute())  # Compute turns Dask DataFrame into a pandas DataFrame if necessary\n",
    "\n",
    "# Predict and evaluate using Dask\n",
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "609ba9bf-597d-4ec6-aafb-2f219b497ac6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to get feature names from ColumnTransformer\n",
    "def get_feature_names(column_transformer):\n",
    "    output_features = []\n",
    "    for name, transformer, features in column_transformer.transformers_:\n",
    "        if name != 'remainder':\n",
    "            if hasattr(transformer, 'get_feature_names_out'):\n",
    "                transformer_features = transformer.get_feature_names_out(features)\n",
    "            else:\n",
    "                transformer_features = [f\"{name}_{f}\" for f in features]\n",
    "            output_features.extend(transformer_features)\n",
    "        else:\n",
    "            output_features.extend(features)\n",
    "    return output_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0083ba27-b91a-4f44-a0d8-34593bd209ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "# Define the data types for each column to optimize memory usage\n",
    "dtype_spec = {\n",
    "    'Em_on_target': 'category',\n",
    "    'Fuel consumption': 'float32',\n",
    "    'Engine_cm3': 'float32',\n",
    "    'Electric range (km)': 'float32',\n",
    "    'Kg_veh': 'float32',\n",
    "    'Test_mass': 'float32',\n",
    "    'Power_KW': 'float32',\n",
    "    'El_Consumpt_whkm': 'float32',\n",
    "    'Energy': 'category',\n",
    "    'Fuel_mode': 'category',\n",
    "    'Brand': 'category',\n",
    "    'Veh_type': 'category',\n",
    "    'Veh_Model': 'category'\n",
    "}\n",
    "\n",
    "columns = ['Em_on_target', 'Fuel consumption', 'Engine_cm3', 'Electric range (km)', 'Kg_veh', 'Test_mass', 'Power_KW', 'El_Consumpt_whkm', 'Energy', 'Fuel_mode', 'Brand', 'Veh_type', 'Veh_Model']\n",
    "dfdt = dd.read_csv('/Users/livalacaisse/Documents/DataScience/CO2/000-C02 First Delivery/Concatenate/PT_FR_ES_IT.csv', usecols=columns, dtype=dtype_spec, assume_missing=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1551372b-c687-45d2-8f6a-9eb83824f4f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'>\n",
      "Columns: 13 entries, Veh_type to Em_on_target\n",
      "dtypes: category(6), float32(7)"
     ]
    }
   ],
   "source": [
    "dfdt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "213c1f0f-a2c0-4c04-91af-8607c86b02ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split data\n",
    "X = dfdt.drop('Em_on_target', axis=1)\n",
    "y = dfdt['Em_on_target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff742afc-4ff5-4724-8a7c-2d83d4ff4976",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define OneHotEncoder and ColumnTransformer for Dask\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=True)  # Dask works better with sparse output\n",
    "categorical_features = ['Energy', 'Fuel_mode', 'Brand', 'Veh_type', 'Veh_Model']\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', one_hot_encoder, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create and fit the modeling pipeline\n",
    "pipeline = make_pipeline(\n",
    "    preprocessor,\n",
    "    Incremental(DecisionTreeClassifier(random_state=42))\n",
    ")\n",
    "\n",
    "# Because Dask does not support SelectKBest with a DecisionTree directly, we may have to adjust the pipeline or drop SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f61b9001-37a7-4ef2-b44f-8ec4e442e719",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'take'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:862\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 862\u001b[0m     tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ready_batches\u001b[38;5;241m.\u001b[39mget(block\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    863\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m queue\u001b[38;5;241m.\u001b[39mEmpty:\n\u001b[1;32m    864\u001b[0m     \u001b[38;5;66;03m# slice the iterator n_jobs * batchsize items at a time. If the\u001b[39;00m\n\u001b[1;32m    865\u001b[0m     \u001b[38;5;66;03m# slice returns less than that, then the current batchsize puts\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    868\u001b[0m     \u001b[38;5;66;03m# accordingly to distribute evenly the last items between all\u001b[39;00m\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# workers.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/queue.py:168\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[0;32m--> 168\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mEmpty\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m client \u001b[38;5;241m=\u001b[39m Client()  \u001b[38;5;66;03m# Starts a local Dask client\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m joblib\u001b[38;5;241m.\u001b[39mparallel_backend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdask\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Train the model using the Dask backend\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     pipeline\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/pipeline.py:401\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \n\u001b[1;32m    377\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    400\u001b[0m fit_params_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_fit_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m--> 401\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_steps)\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/pipeline.py:359\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    357\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m fit_transform_one_cached(\n\u001b[1;32m    360\u001b[0m     cloned_transformer,\n\u001b[1;32m    361\u001b[0m     X,\n\u001b[1;32m    362\u001b[0m     y,\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    364\u001b[0m     message_clsname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    365\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(step_idx),\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_steps[name],\n\u001b[1;32m    367\u001b[0m )\n\u001b[1;32m    368\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/memory.py:349\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/pipeline.py:893\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 893\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    895\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:727\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_column_callables(X)\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_remainder(X)\n\u001b[0;32m--> 727\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_transform(X, y, _fit_transform_one)\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fitted_transformers([])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:658\u001b[0m, in \u001b[0;36mColumnTransformer._fit_transform\u001b[0;34m(self, X, y, func, fitted, column_as_strings)\u001b[0m\n\u001b[1;32m    652\u001b[0m transformers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(\n\u001b[1;32m    654\u001b[0m         fitted\u001b[38;5;241m=\u001b[39mfitted, replace_strings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, column_as_strings\u001b[38;5;241m=\u001b[39mcolumn_as_strings\n\u001b[1;32m    655\u001b[0m     )\n\u001b[1;32m    656\u001b[0m )\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 658\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)(\n\u001b[1;32m    659\u001b[0m         delayed(func)(\n\u001b[1;32m    660\u001b[0m             transformer\u001b[38;5;241m=\u001b[39mclone(trans) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fitted \u001b[38;5;28;01melse\u001b[39;00m trans,\n\u001b[1;32m    661\u001b[0m             X\u001b[38;5;241m=\u001b[39m_safe_indexing(X, column, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    662\u001b[0m             y\u001b[38;5;241m=\u001b[39my,\n\u001b[1;32m    663\u001b[0m             weight\u001b[38;5;241m=\u001b[39mweight,\n\u001b[1;32m    664\u001b[0m             message_clsname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnTransformer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    665\u001b[0m             message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(name, idx, \u001b[38;5;28mlen\u001b[39m(transformers)),\n\u001b[1;32m    666\u001b[0m         )\n\u001b[1;32m    667\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx, (name, trans, column, weight) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(transformers, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    668\u001b[0m     )\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[1;32m   1078\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1085\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1088\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:873\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    870\u001b[0m n_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_effective_n_jobs\n\u001b[1;32m    871\u001b[0m big_batch_size \u001b[38;5;241m=\u001b[39m batch_size \u001b[38;5;241m*\u001b[39m n_jobs\n\u001b[0;32m--> 873\u001b[0m islice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(itertools\u001b[38;5;241m.\u001b[39mislice(iterator, big_batch_size))\n\u001b[1;32m    874\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(islice) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    875\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:59\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Capture the thread-local scikit-learn configuration at the time\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Parallel.__call__ is issued since the tasks can be dispatched\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# in a different thread depending on the backend and on the value of\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# pre_dispatch and n_jobs.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m---> 59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:661\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    652\u001b[0m transformers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(\n\u001b[1;32m    654\u001b[0m         fitted\u001b[38;5;241m=\u001b[39mfitted, replace_strings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, column_as_strings\u001b[38;5;241m=\u001b[39mcolumn_as_strings\n\u001b[1;32m    655\u001b[0m     )\n\u001b[1;32m    656\u001b[0m )\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)(\n\u001b[1;32m    659\u001b[0m         delayed(func)(\n\u001b[1;32m    660\u001b[0m             transformer\u001b[38;5;241m=\u001b[39mclone(trans) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fitted \u001b[38;5;28;01melse\u001b[39;00m trans,\n\u001b[0;32m--> 661\u001b[0m             X\u001b[38;5;241m=\u001b[39m_safe_indexing(X, column, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    662\u001b[0m             y\u001b[38;5;241m=\u001b[39my,\n\u001b[1;32m    663\u001b[0m             weight\u001b[38;5;241m=\u001b[39mweight,\n\u001b[1;32m    664\u001b[0m             message_clsname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnTransformer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    665\u001b[0m             message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(name, idx, \u001b[38;5;28mlen\u001b[39m(transformers)),\n\u001b[1;32m    666\u001b[0m         )\n\u001b[1;32m    667\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx, (name, trans, column, weight) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(transformers, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    668\u001b[0m     )\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/__init__.py:354\u001b[0m, in \u001b[0;36m_safe_indexing\u001b[0;34m(X, indices, axis)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    349\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpecifying the columns using strings is only supported for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    350\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas DataFrames\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    351\u001b[0m     )\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pandas_indexing(X, indices, indices_dtype, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _array_indexing(X, indices, indices_dtype, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/__init__.py:196\u001b[0m, in \u001b[0;36m_pandas_indexing\u001b[0;34m(X, key, key_dtype, axis)\u001b[0m\n\u001b[1;32m    191\u001b[0m     key \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(key)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key_dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(key)):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# using take() instead of iloc[] ensures the return value is a \"proper\"\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# copy that will not raise SettingWithCopyWarning\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X\u001b[38;5;241m.\u001b[39mtake(key, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;66;03m# check whether we should index with loc or iloc\u001b[39;00m\n\u001b[1;32m    199\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39miloc \u001b[38;5;28;01mif\u001b[39;00m key_dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m X\u001b[38;5;241m.\u001b[39mloc\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/dask/dataframe/core.py:5136\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   5134\u001b[0m     \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key)\n\u001b[1;32m   5135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 5136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m key)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'take'"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client()  # Starts a local Dask client\n",
    "with joblib.parallel_backend('dask'):\n",
    "    # Train the model using the Dask backend\n",
    "    pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e070628d-fcba-4e19-ba7f-d0f4be7b3ba5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict and evaluate\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8494ff-e925-4356-a12b-8293017beba8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define and execute GridSearchCV\n",
    "param_grid = {\n",
    "    'feature_selection__k': [10, 15],\n",
    "    'classifier__max_depth': [None, 5, 10],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 5],\n",
    "    'classifier__ccp_alpha': [0.0, 0.01]\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=2, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validated score:\", grid_search.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4a107f-eed0-44f1-ac0b-3413d7853d75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract features from the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "feature_selector = best_model.named_steps['feature_selection']\n",
    "features_selected = feature_selector.get_support()\n",
    "transformed_features = get_feature_names(best_model.named_steps['preprocessor'])\n",
    "important_features = [feature for feature, selected in zip(transformed_features, features_selected) if selected]\n",
    "print(\"Important features:\", important_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a339e9cc-7ea8-40ca-a8ce-bd6134adf5cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Feature importance\n",
    "import numpy as np\n",
    "\n",
    "def get_final_feature_names(pipeline, input_features, dtype_dict):\n",
    "    # Create a dummy DataFrame with appropriate data types\n",
    "    dummy_data = pd.DataFrame(np.zeros(shape=(1, len(input_features))), columns=input_features)\n",
    "    for col, dtype in dtype_dict.items():\n",
    "        dummy_data[col] = dummy_data[col].astype(dtype)\n",
    "\n",
    "    # Transform the dummy data through the pipeline's preprocessing steps\n",
    "    pipeline.named_steps['preprocessor'].transform(dummy_data)\n",
    "\n",
    "    # Retrieve feature names from the pipeline's ColumnTransformer\n",
    "    feature_names = pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "\n",
    "    # Adjust for feature selection\n",
    "    if 'feature_selection' in pipeline.named_steps:\n",
    "        support_mask = pipeline.named_steps['feature_selection'].get_support()\n",
    "        selected_features = feature_names[support_mask]\n",
    "    else:\n",
    "        selected_features = feature_names\n",
    "\n",
    "    return selected_features\n",
    "\n",
    "# Define data types as they appear in X_train for dummy data creation\n",
    "dtype_dict = {col: X_train[col].dtype for col in X_train.columns}\n",
    "\n",
    "# Retrieve correct feature names\n",
    "final_features = get_final_feature_names(best_model, X_train.columns, dtype_dict)\n",
    "\n",
    "# Now pair these with the feature importances\n",
    "importances = pd.DataFrame({\n",
    "    \"Feature\": final_features,\n",
    "    \"Importance\": best_model.named_steps['classifier'].feature_importances_\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "print(\"Feature importances:\\n\", importances.head(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdd30dc-f93e-419d-930f-85a8da3792be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Best model parameters\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the OneHotEncoder and ColumnTransformer\n",
    "categorical_features = ['Energy', 'Fuel_mode', 'Brand', 'Veh_type']  # same categorical columns as before\n",
    "one_hot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', one_hot_encoder, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create the modeling pipeline including preprocessing\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Parameter grid\n",
    "param_grid = {\n",
    "    'classifier__min_samples_leaf': [1, 4, 6, 10],\n",
    "    'classifier__min_samples_split': [2, 10, 20],\n",
    "    'classifier__max_depth': [None, 5, 10, 15],\n",
    "    'classifier__ccp_alpha': [0.0, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "# Initialize and fit the GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=2, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model and parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluate the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Extracting feature importance from the best model\n",
    "feature_importances = best_model.named_steps['classifier'].feature_importances_\n",
    "transformed_features = best_model.named_steps['preprocessor'].get_feature_names_out()\n",
    "\n",
    "# Pairing feature names with their importances\n",
    "importances = pd.DataFrame({\n",
    "    'Feature': transformed_features,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "print(importances.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faebb2f9-ec4e-4c92-8bef-349bad985b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c94dcd1-95c2-4de7-9a7b-23b0ba61d7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# print the JS visualization code to the notebook\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caf4596-fd8e-4c60-9bfb-942ac8a7a5c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139a1fe7-6663-4486-9486-0fae92920fe4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to extract feature names\n",
    "def get_feature_names(column_transformer):\n",
    "    col_names = []\n",
    "    for name, transformer, cols in column_transformer.transformers_:\n",
    "        if name == 'remainder' and transformer == 'passthrough':\n",
    "            col_names.extend(cols)\n",
    "        elif hasattr(transformer, 'get_feature_names_out'):\n",
    "            col_names.extend(transformer.get_feature_names_out())\n",
    "        else:\n",
    "            col_names.extend(cols)\n",
    "    return col_names\n",
    "\n",
    "# Assuming your pipeline has been fitted\n",
    "fitted_preprocessor = pipeline.named_steps['preprocessor']\n",
    "X_train_transformed = fitted_preprocessor.transform(X_train)\n",
    "feature_names = get_feature_names(fitted_preprocessor)\n",
    "X_train_transformed_df = pd.DataFrame(X_train_transformed, columns=feature_names)\n",
    "\n",
    "## check number of feature names\n",
    "print(\"Number of feature names:\", len(feature_names))\n",
    "print(\"Number of features in transformed data:\", X_train_transformed_df.shape[1])\n",
    "\n",
    "# Check if any mismatch in numbers\n",
    "assert len(feature_names) == X_train_transformed_df.shape[1], \"Mismatch in number of features!\"\n",
    "\n",
    "# Initialize SHAP Explainer\n",
    "explainer = shap.Explainer(pipeline.named_steps['classifier'], X_train_transformed_df)\n",
    "\n",
    "# Generate SHAP values (Ensure this returns an Explanation object)\n",
    "shap_values = explainer(X_train_transformed_df)\n",
    "\n",
    "# Initialize JavaScript for SHAP in Jupyter Notebooks\n",
    "shap.initjs()\n",
    "\n",
    "# Check if SHAP values are in a list (indicative of multi-class outputs)\n",
    "if isinstance(shap_values, list):\n",
    "    # Assuming we're interested in the first class (adjust as necessary)\n",
    "    class_index = 0\n",
    "    shap.summary_plot(shap_values[class_index], X_train_transformed_df, feature_names=feature_names)\n",
    "else:\n",
    "    shap.summary_plot(shap_values, X_train_transformed_df, feature_names=feature_names)\n",
    "# Simple bar plot to display the mean absolute values of SHAP for all features\n",
    "shap.summary_plot(shap_values, X_train_transformed_df, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219f7a28-bfb4-4bf6-a5c1-bb28a4d12b48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Simple bar plot to display the mean absolute values of SHAP for all features\n",
    "shap.summary_plot(shap_values, X_train_transformed_df, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4e679c-a21a-4163-9761-a667106c0c63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Debug feature names\n",
    "print(type(feature_names))\n",
    "print(feature_names[:10])\n",
    "\n",
    "# Initialize SHAP Explainer\n",
    "explainer = shap.Explainer(pipeline.named_steps['classifier'], X_train_transformed_df)\n",
    "shap_values = explainer(X_train_transformed_df)\n",
    "\n",
    "# Initialize JavaScript for visualization in Jupyter Notebooks\n",
    "shap.initjs()\n",
    "\n",
    "# Display the bar plot using the correct function and ensuring shap_values is an Explanation object\n",
    "if isinstance(shap_values, shap.Explanation):\n",
    "    shap.plots.bar(shap_values)\n",
    "else:\n",
    "    print(\"shap_values must be an shap.Explanation object for shap.plots.bar().\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749bd20d-2922-4651-bb19-38f52de19471",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(pipeline)\n",
    "shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e37f8a0-23f1-4fb7-851e-64c66d430eaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# draw model\n",
    "dot_data = export_graphviz(\n",
    "    single_split_model,\n",
    "    out_file=None,\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    special_characters=True,\n",
    ")\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db326b5-dbb3-46f5-b84e-e84120ab74d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
