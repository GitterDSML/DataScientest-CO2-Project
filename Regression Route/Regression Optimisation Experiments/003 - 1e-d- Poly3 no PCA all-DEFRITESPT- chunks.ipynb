{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96aa7c8f-434a-49f4-83c4-35f88327580d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score, KFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9ed7dd2-d57b-4c53-8879-db010cabf009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "fr = r\"/Users/livalacaisse/Documents/DataScience/CO2/000-C02 First Delivery/Cleaned_countries/FR_Cleaned.csv\"\n",
    "de = r\"/Users/livalacaisse/Documents/DataScience/CO2/000-C02 First Delivery/Cleaned_countries/Cl_no_FR/DT_Cleaned.csv\"\n",
    "es = r\"/Users/livalacaisse/Documents/DataScience/CO2/000-C02 First Delivery/Cleaned_countries/Cl_no_FR/ES_Cleaned.csv\"\n",
    "it = r\"/Users/livalacaisse/Documents/DataScience/CO2/000-C02 First Delivery/Cleaned_countries/Cl_no_FR/IT_Cleaned.csv\"\n",
    "pt = r\"/Users/livalacaisse/Documents/DataScience/CO2/000-C02 First Delivery/Cleaned_countries/Cl_no_FR/PT_Cleaned.csv\"\n",
    "\n",
    "# Data types specification\n",
    "dtype_spec = {\n",
    "    'Em_on_target': 'int64',\n",
    "    'Fuel consumption': 'float32',\n",
    "    'Engine_cm3': 'float32',\n",
    "    'Kg_veh': 'float32',\n",
    "    'Test_mass': 'float32',\n",
    "    'Power_KW': 'float32',\n",
    "    'El_Consumpt_whkm': 'float32',\n",
    "    'Energy': 'category',\n",
    "    'Fuel_mode': 'category',\n",
    "    'Brand': 'category',\n",
    "    'Veh_type': 'category',\n",
    "    'Veh_Model': 'category',\n",
    "    'Version': 'category',\n",
    "    'Veh_Category': 'category',\n",
    "    'year': 'int64',\n",
    "    'Country': 'category'\n",
    "}\n",
    "\n",
    "# Define the features for each energy type\n",
    "features_dict = {\n",
    "    'petrol': ['Fuel consumption', 'Wheelbase_mm', 'Engine_cm3', 'Power_KW', 'Axle_width_steer_mm', \n",
    "               'Test_mass', 'Axle_width_other_mm', 'Eco-innovation program', 'Kg_veh', 'year', \n",
    "               'Erwltp (g/km)', 'El_Consumpt_whkm'],\n",
    "    'diesel': ['Wheelbase_mm', 'Axle_width_other_mm', 'Power_KW', 'Test_mass', 'Axle_width_steer_mm', \n",
    "               'Kg_veh', 'Engine_cm3', 'Fuel consumption', 'year', 'Erwltp (g/km)', 'Eco-innovation program', \n",
    "               'El_Consumpt_whkm'],\n",
    "    'lpg': ['Kg_veh', 'Fuel consumption', 'Axle_width_steer_mm', 'Axle_width_other_mm', 'year', 'Test_mass', \n",
    "            'Power_KW', 'Erwltp (g/km)', 'Engine_cm3', 'Wheelbase_mm', 'Eco-innovation program', \n",
    "            'Electric range (km)'],\n",
    "    'hybrid petrol': ['Engine_cm3', 'Axle_width_other_mm', 'Axle_width_steer_mm', 'Fuel consumption', \n",
    "                     'El_Consumpt_whkm', 'Power_KW', 'Electric range (km)', 'year', 'Wheelbase_mm', \n",
    "                     'Eco-innovation program', 'Test_mass', 'Kg_veh'],\n",
    "    'hybrid diesel': ['El_Consumpt_whkm', 'Axle_width_other_mm', 'Electric range (km)', 'Axle_width_steer_mm', \n",
    "                     'year', 'Kg_veh', 'Test_mass', 'Fuel consumption', 'Wheelbase_mm', 'Eco-innovation program', \n",
    "                     'Erwltp (g/km)', 'Power_KW']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88a000a6-d0cc-45cc-8ab7-6c76f2d6c78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load data in chunks\n",
    "def load_data_in_chunks(files, dtype_spec, chunksize=10000):\n",
    "    df_list = []\n",
    "    for file in files:\n",
    "        for chunk in pd.read_csv(file, dtype=dtype_spec, chunksize=chunksize):\n",
    "            df_list.append(chunk)\n",
    "    return pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "485ae8e2-a9d8-4c47-af63-71e74eaf1c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare data for regression\n",
    "def prepare_data(df, energy_types, features_dict):\n",
    "    if isinstance(energy_types, list):\n",
    "        dfs = []\n",
    "        for energy in energy_types:\n",
    "            df_energy = df[df['Energy'] == energy].copy()\n",
    "            if df_energy.empty:\n",
    "                print(f\"No data for energy type: {energy}\")\n",
    "                continue\n",
    "            df_energy = df_energy[features_dict[energy] + ['CO2_wltp']]\n",
    "            dfs.append(df_energy)\n",
    "        if not dfs:\n",
    "            return pd.DataFrame(), pd.Series(), []\n",
    "        df_combined = pd.concat(dfs, axis=0)\n",
    "        features = [feat for energy in energy_types for feat in features_dict[energy]]\n",
    "        features = list(set(features))  # Remove duplicates\n",
    "    else:\n",
    "        df_combined = df[df['Energy'] == energy_types].copy()\n",
    "        if df_combined.empty:\n",
    "            print(f\"No data for energy type: {energy_types}\")\n",
    "            return pd.DataFrame(), pd.Series(), []\n",
    "        features = features_dict[energy_types]\n",
    "    \n",
    "    X = df_combined[features]\n",
    "    y = df_combined['CO2_wltp']\n",
    "    \n",
    "    return X, y, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be1ad558-da46-41e6-9d7b-2578a819f970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression function with RandomizedSearchCV and polynomial features\n",
    "def ridge_regression_with_random_search(df, energy_types, features_dict, alphas, degree=3):\n",
    "    # Prepare data\n",
    "    X, y, feature_names = prepare_data(df, energy_types, features_dict)\n",
    "    \n",
    "    if X.empty or y.empty:\n",
    "        print(f\"No data available for energy types: {energy_types}\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Handle missing values\n",
    "    X = X.fillna(X.mean())  # Avoid SettingWithCopyWarning\n",
    "    \n",
    "    # Polynomial Features\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    \n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_poly)\n",
    "    \n",
    "    # Train-Test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Parameter grid for alpha\n",
    "    param_grid = {'alpha': alphas}\n",
    "    \n",
    "    # Ridge Regression with RandomizedSearchCV\n",
    "    ridge = Ridge()\n",
    "    random_search = RandomizedSearchCV(ridge, param_distributions=param_grid, n_iter=100, cv=5, random_state=42, n_jobs=-1)\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Best model\n",
    "    best_ridge = random_search.best_estimator_\n",
    "    best_alpha = random_search.best_params_['alpha']\n",
    "    \n",
    "    # Predictions with best model\n",
    "    y_pred_best = best_ridge.predict(X_test)\n",
    "    \n",
    "    # Evaluation of best model\n",
    "    best_r2 = r2_score(y_test, y_pred_best)\n",
    "    mse_best = mean_squared_error(y_test, y_pred_best)\n",
    "    \n",
    "    # Results reporting\n",
    "    print(f\"Energy Type: {'All' if isinstance(energy_types, list) else energy_types}\")\n",
    "    print(f\"Best Alpha: {best_alpha}\")\n",
    "    print(f\"Best R^2 Score: {best_r2}\")\n",
    "    print(f\"Mean Squared Error: {mse_best}\")\n",
    "    \n",
    "    # Save the best model\n",
    "    model_filename = f\"best_model_{energy_types.replace(' ', '_') if isinstance(energy_types, str) else 'all'}.joblib\"\n",
    "    joblib.dump(best_ridge, model_filename)\n",
    "    print(f\"Model saved as {model_filename}\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "    \n",
    "    # Scatter plot of actual vs predicted for best model\n",
    "    sns.scatterplot(x=y_test, y=y_pred_best, ax=axes[0])\n",
    "    axes[0].set_xlabel('Actual CO2_wltp')\n",
    "    axes[0].set_ylabel('Predicted CO2_wltp')\n",
    "    axes[0].set_title(f'Regression Results for {\"All Energies\" if isinstance(energy_types, list) else energy_types}')\n",
    "    \n",
    "    # R^2 scores vs alpha plot\n",
    "    results_df = pd.DataFrame(random_search.cv_results_)\n",
    "    sns.lineplot(x='param_alpha', y='mean_test_score', data=results_df, ax=axes[1])\n",
    "    axes[1].set_xlabel('Alpha')\n",
    "    axes[1].set_ylabel('Mean CV R^2 Score')\n",
    "    axes[1].set_title('Mean CV R^2 Score vs Alpha')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print feature importance for best model\n",
    "    original_features = poly.get_feature_names_out(feature_names)\n",
    "    coef_df = pd.DataFrame({'Feature': original_features, 'Coefficient': np.abs(best_ridge.coef_)})\n",
    "    coef_df = coef_df.sort_values(by='Coefficient', ascending=False)\n",
    "    \n",
    "    print(\"Feature Importances:\")\n",
    "    for feature, coef in zip(coef_df['Feature'], coef_df['Coefficient']):\n",
    "        print(f\"{feature}: {coef}\")\n",
    "    \n",
    "    # Visualization of feature importances\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Coefficient', y='Feature', data=coef_df)\n",
    "    plt.axvline(x=0, color='red', linestyle='--')\n",
    "    plt.title('Feature Coefficients')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return best_ridge, best_r2, mse_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "306e0e0f-d63a-4fae-9f14-602592188ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data in chunks\n",
    "files = [fr, de, es]  # Add it and pt if needed\n",
    "df_clean = load_data_in_chunks(files, dtype_spec)\n",
    "\n",
    "# Drop specified columns\n",
    "drop = [\"Unnamed: 0\", \"Em_on_target\"]\n",
    "df_clean = df_clean.drop(drop, axis=1)\n",
    "#df_clean.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b74f23b-24db-4be1-ac30-555000e16546",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m energy, features \u001b[38;5;129;01min\u001b[39;00m features_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m----> 5\u001b[0m     results[energy] \u001b[38;5;241m=\u001b[39m ridge_regression_with_random_search(df_clean, energy, features_dict, alphas, degree\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 19\u001b[0m, in \u001b[0;36mridge_regression_with_random_search\u001b[0;34m(df, energy_types, features_dict, alphas, degree)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Standardize the features\u001b[39;00m\n\u001b[1;32m     18\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[0;32m---> 19\u001b[0m X_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(X_poly)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Train-Test Split\u001b[39;00m\n\u001b[1;32m     22\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X_scaled, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:878\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    877\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:824\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 824\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_fit(X, y, sample_weight)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:861\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m    860\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 861\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m    862\u001b[0m     X,\n\u001b[1;32m    863\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    864\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mFLOAT_DTYPES,\n\u001b[1;32m    865\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    866\u001b[0m     reset\u001b[38;5;241m=\u001b[39mfirst_call,\n\u001b[1;32m    867\u001b[0m )\n\u001b[1;32m    868\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    870\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 565\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m    566\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:921\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    916\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    917\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    918\u001b[0m         )\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 921\u001b[0m         _assert_all_finite(\n\u001b[1;32m    922\u001b[0m             array,\n\u001b[1;32m    923\u001b[0m             input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m    924\u001b[0m             estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m    925\u001b[0m             allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    926\u001b[0m         )\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    929\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:122\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# First try an O(n) time, O(1) space solution for the common case that\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# everything is finite; fall back to O(n) space `np.isinf/isnan` or custom\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Cython implementation to prevent false positives and provide a detailed\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# error message.\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(over\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 122\u001b[0m     first_pass_isfinite \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39misfinite(xp\u001b[38;5;241m.\u001b[39msum(X))\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py:2313\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2310\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   2311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m-> 2313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _wrapreduction(a, np\u001b[38;5;241m.\u001b[39madd, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m, axis, dtype, out, keepdims\u001b[38;5;241m=\u001b[39mkeepdims,\n\u001b[1;32m   2314\u001b[0m                       initial\u001b[38;5;241m=\u001b[39minitial, where\u001b[38;5;241m=\u001b[39mwhere)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py:88\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Apply Ridge Regression for each energy type and all together with polynomial features\n",
    "alphas = np.logspace(-10, 10, 200)\n",
    "results = {}\n",
    "for energy, features in features_dict.items():\n",
    "    results[energy] = ridge_regression_with_random_search(df_clean, energy, features_dict, alphas, degree=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825092c2-2787-44e8-bd2a-26d6e97543a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run regression on all energies together\n",
    "all_energies = list(features_dict.keys())\n",
    "results['all_energies'] = ridge_regression_with_random_search(df_clean, all_energies, features_dict, alphas, degree=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
